{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Perguntas_ver1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaimovi/Analise-de-similaridade-de-Banco-de-Perguntas-por-metodos-NLP/blob/main/Perguntas_ver1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8516e4cfe5225b5c02a18f2be3c31313249b24c3",
        "_cell_guid": "07597ed8-0117-4ce1-b5b4-725ae012da4e",
        "id": "sPPU2PoqPJgt"
      },
      "source": [
        "# Perguntas\n",
        "##A ideia deste notebook estruturar inicialmente todas as etapas de pre processamento necessárias e que deverão ser aplicadas, independente de qual modelo . Estabelecer as etapas de transformação em caixa baixa, retirar stopwords, caracteres especiais e etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HehC7UEBxXk"
      },
      "source": [
        "Carrego as Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7wWYfAQ2jhW"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "xKbWqIX6PJgu"
      },
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "from keras.preprocessing import text, sequence\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7ReY4ZofbIB"
      },
      "source": [
        "# Load Data\n",
        "df=pd.read_excel('3 colunas de PERGUNTA.xlsx', usecols = \"A,C:C\")\n",
        "df= df.rename(columns={'Texto da Pergunta': 'Pergunta'})\n",
        "df= df.rename(columns={'Codigo da Pergunta': 'Codigo'})\n",
        "df= df[:3000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgvp89J9B34b"
      },
      "source": [
        "cabeçalho"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8OhMK9XeDgm"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nbSaLcZyM6s"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jDUxrGkyTyp"
      },
      "source": [
        "# Simplificar o df para uns 10k~5k\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AElLSRfcd_l9"
      },
      "source": [
        "df.head(100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": false,
        "id": "BtfvCXinPJgy"
      },
      "source": [
        "df.tail(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "11317c6e054b4c596c16aff329af629e4939a5a9",
        "_cell_guid": "fc76f9c9-1c5a-4963-b3a9-50fd7db6746f",
        "trusted": false,
        "id": "WvV2iXFNPJg0"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6aa550f242ac6d825be73e3beffc7d2446a0d1bd",
        "_cell_guid": "f4f393bc-9b20-4468-a548-07619c807d21",
        "id": "LrxzGXC3PJg3"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_t6rBjpraoL"
      },
      "source": [
        "print(df.Pergunta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj8k3xyRCXzT"
      },
      "source": [
        "##Nossa conhecida WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "da834b058cbbf98856041487f7352d9f1d36f451",
        "_cell_guid": "8065251c-02b3-4156-9cf0-5204828012e7",
        "trusted": true,
        "id": "sWGgzNyJPJhC"
      },
      "source": [
        "# word cloud\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "\n",
        "def clean_text(s):\n",
        "    if type(s)!=str:\n",
        "     return \"\"\n",
        "    s = re.sub(r'http\\S+', '', s)\n",
        "    s = re.sub('(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)', ' ', s)\n",
        "    s = re.sub(r'@\\S+', '', s)\n",
        "    s = re.sub('&amp', ' ', s)\n",
        "    s = re.sub('DO', ' ', s)\n",
        "    s = re.sub(r'À', '', s)\n",
        "    return s\n",
        "df['Pergunta'] = df['Pergunta'].apply(clean_text)\n",
        "\n",
        "caixa_baixa = df['Pergunta'].to_string().lower()   \n",
        "wordcloud = WordCloud(\n",
        "    collocations=False,\n",
        "    relative_scaling=0.5,\n",
        "    stopwords=set(stopwords.words('portuguese'))).generate(caixa_baixa)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75IKQrzbhqfg"
      },
      "source": [
        "auxiliar = df['Pergunta'].str.lower()\n",
        "df['Pergunta']= auxiliar\n",
        "df.head(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qxrljs2qxFr"
      },
      "source": [
        "df.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyjmXCfTpdWh"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    try:\n",
        "      nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    except:\n",
        "      print(input_str)\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    return str(only_ascii)\n",
        "\n",
        "auxiliar = [remove_accents(sentence).replace('b\\'','') for sentence in df['Pergunta']]\n",
        "auxiliar2 = [sentence.replace('\\'','') for sentence in auxiliar]\n",
        "df['Pergunta']= auxiliar2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXGehX0drJGx"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX4VGwykzk0R"
      },
      "source": [
        "todas_stopwords = stopwords.words('portuguese')\n",
        "todas_stopwords.append('para')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrCf_r8SkqDY"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "# inserir nova stopword\n",
        "# stopwords.append('existe')\n",
        "print(stopwords.words('portuguese'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dLbjxvNtbGz"
      },
      "source": [
        "# PROCURAR TEXTO PARA REMOVER STOPWORDS\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0v83jwTX9BK"
      },
      "source": [
        "# df = df.explode(\"Pergunta\", ignore_index=True)\n",
        "# df.rename(columns={\"Codigo\": \"Pergunta\"}, inplace=True)\n",
        "# df.index.name = \"Code_ID\"\n",
        "# df.head()\n",
        "# df.to_csv(\"Perguntas Tokenized.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5v1C238fiaE"
      },
      "source": [
        "#Para o caso de bag of words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxb8LkWTu-LU"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents\n",
        "vect = TfidfVectorizer(min_df=1, stop_words=todas_stopwords)\n",
        "tfidf = vect.fit_transform(df['Pergunta'][:3000])\n",
        "pairwise_similarity = tfidf * tfidf.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDAQVR6LvoKn"
      },
      "source": [
        "pairwise_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_QUvyygwL4v"
      },
      "source": [
        "pd.DataFrame(pairwise_similarity.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhyCgIf00bAM"
      },
      "source": [
        "resultado = pd.DataFrame(pairwise_similarity.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rWg_TxJ_mfs"
      },
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "   \n",
        "\n",
        "  \n",
        "  \n",
        "print (resultado)\n",
        "  \n",
        "print (\"\\nclipped arr1 : \\n\", stats.threshold(\n",
        "        resultado, threshmin = 0.7 , threshmax = 1, newval = 0.85))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpIDt-Ui0mzr"
      },
      "source": [
        "resultado.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clzaii2J1AnE"
      },
      "source": [
        "resultado.to_excel('resultado.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ozCKpcsxEO6"
      },
      "source": [
        "df['Pergunta'][86], df['Pergunta'][514],df['Pergunta'][133], df['Pergunta'][112]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbGQ4Qguxr8v"
      },
      "source": [
        "# Definir um valor de threshold para classificar como semelhante\n",
        "\n",
        "\n",
        "# Remover as perguntas semelhantes do conjunto\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjSlUiz_yynE"
      },
      "source": [
        "\n",
        "resultado[(resultado > 0.99)].any(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8VhRuZ2uDh1"
      },
      "source": [
        "to_drop = []\n",
        "par_perguntas =[]\n",
        "\n",
        "for column in resultado.columns:\n",
        " for index in resultado.index:\n",
        "  if resultado.loc[index, column] > 0.95 and index != column:\n",
        "   to_drop.append([index,column])\n",
        "   par_perguntas.append([df['Pergunta'][index],df['Pergunta'][column]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYXDJBugwfKp"
      },
      "source": [
        "len(to_drop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKfH3lE3vDCr"
      },
      "source": [
        "print(to_drop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsvg9q9L7g6V"
      },
      "source": [
        "to_drop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SXOGqBD7SEp"
      },
      "source": [
        "par_perguntas[30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLCK0BUWyhoD"
      },
      "source": [
        "par_perguntas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-jI-8dfxSJH"
      },
      "source": [
        "\n",
        "df['Pergunta'][514],df['Pergunta'][86]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ-O6mUEyy6u"
      },
      "source": [
        "# Aplicação de modelo transformer para similaridade\n",
        "\n",
        "\n",
        "https://huggingface.co/neuralmind/bert-base-portuguese-cased\n",
        "\n",
        "https://github.com/neuralmind-ai/portuguese-bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhDCdUsxzsm_"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e4RI2DEztd_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJrO1VneFoVk"
      },
      "source": [
        "# ======= SEM UTILIDADE ======="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXRcZSH0YWna"
      },
      "source": [
        "def get_corpus(caixa_baixa):\n",
        "    words = []\n",
        "    for i in caixa_baixa:\n",
        "        for j in i.split():\n",
        "            words.append(j.strip())\n",
        "    return words\n",
        "corpus = get_corpus(df.Pergunta)\n",
        "corpus[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TRW_VvdZDWx"
      },
      "source": [
        "from collections import Counter\n",
        "counter = Counter(corpus)\n",
        "most_common = counter.most_common(15)\n",
        "most_common = dict(most_common)\n",
        "most_common"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1hrk30eZYhy"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "def get_top_text_ngrams(corpus, n, g):\n",
        "    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hALI9n3varc5"
      },
      "source": [
        "plt.figure(figsize = (16,9))\n",
        "most_common_uni = get_top_text_ngrams(df.clean,10,1)\n",
        "most_common_uni = dict(most_common_uni)\n",
        "sns.barplot(x=list(most_common_uni.values()),y=list(most_common_uni.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NBDm40YbIMG"
      },
      "source": [
        "plt.figure(figsize = (16,9))\n",
        "most_common_bi = get_top_text_ngrams(df.clean,10,2)\n",
        "most_common_bi = dict(most_common_bi)\n",
        "sns.barplot(x=list(most_common_bi.values()),y=list(most_common_bi.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXErtZ7QbTYO"
      },
      "source": [
        "plt.figure(figsize = (16,9))\n",
        "most_common_tri = get_top_text_ngrams(df.clean,10,7)\n",
        "most_common_tri = dict(most_common_tri)\n",
        "sns.barplot(x=list(most_common_tri.values()),y=list(most_common_tri.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSAqnrb2DmKz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}