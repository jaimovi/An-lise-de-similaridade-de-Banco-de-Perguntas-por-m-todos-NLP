{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Perguntas_ver1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaimovi/Analise-de-similaridade-de-Banco-de-Perguntas-por-metodos-NLP/blob/main/Perguntas_ver1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8516e4cfe5225b5c02a18f2be3c31313249b24c3",
        "_cell_guid": "07597ed8-0117-4ce1-b5b4-725ae012da4e",
        "id": "sPPU2PoqPJgt"
      },
      "source": [
        "# Perguntas\n",
        "##Este notebook estrutura inicialmente todas as etapas de pre processamento necessárias e que deverão ser aplicadas, independente de qual modelo . Estabelecer as etapas de transformação em caixa baixa, retirar stopwords, caracteres especiais e etc.Em seguida utilizaremos ainda neste Notebook o método de comparação por bag of words, calculando os vetores da sentenção, limitando o número de linhas, considerando a limitação de recursos computacionais do Colab. A opção então foi de importar todas as linhas do Banco de perguntas proposto, com 170000 perguntas e apenas buscar similaridades com as primeiras 5000 perguntas.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HehC7UEBxXk"
      },
      "source": [
        "Carrego as Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7wWYfAQ2jhW"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "xKbWqIX6PJgu"
      },
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "from keras.preprocessing import text, sequence\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so-I6dJ8LE7b"
      },
      "source": [
        "#Importar a base de Perguntas, Limitando a 5000 linhas iniciais."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7ReY4ZofbIB"
      },
      "source": [
        "# Load Data\n",
        "df=pd.read_excel('3 colunas de PERGUNTA.xlsx', usecols = \"A,C:C\")\n",
        "df= df.rename(columns={'Texto da Pergunta': 'Pergunta'})\n",
        "df= df.rename(columns={'Codigo da Pergunta': 'Codigo'})\n",
        "df= df[:5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgvp89J9B34b"
      },
      "source": [
        "cabeçalho"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8OhMK9XeDgm"
      },
      "source": [
        "#Examinando a Importação\n",
        "#Observar que toda pergunta possui um código unico, oriundo da base de dados.\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nbSaLcZyM6s"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": false,
        "id": "BtfvCXinPJgy"
      },
      "source": [
        "df.tail(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "11317c6e054b4c596c16aff329af629e4939a5a9",
        "_cell_guid": "fc76f9c9-1c5a-4963-b3a9-50fd7db6746f",
        "trusted": false,
        "id": "WvV2iXFNPJg0"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6aa550f242ac6d825be73e3beffc7d2446a0d1bd",
        "_cell_guid": "f4f393bc-9b20-4468-a548-07619c807d21",
        "id": "LrxzGXC3PJg3"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_t6rBjpraoL"
      },
      "source": [
        "#Há muita incidência de perguntas em Caixa alta e caixa baixa.\n",
        "print(df.Pergunta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj8k3xyRCXzT"
      },
      "source": [
        "##Transformar a coluna em análise Pergunta para minusculo, eliminar os valores nulos, stopwords, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75IKQrzbhqfg"
      },
      "source": [
        "auxiliar = df['Pergunta'].str.lower()\n",
        "df['Pergunta']= auxiliar\n",
        "df.head(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qxrljs2qxFr"
      },
      "source": [
        "df.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyjmXCfTpdWh"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    try:\n",
        "      nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    except:\n",
        "      print(input_str)\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    return str(only_ascii)\n",
        "\n",
        "auxiliar = [remove_accents(sentence).replace('b\\'','') for sentence in df['Pergunta']]\n",
        "auxiliar2 = [sentence.replace('\\'','') for sentence in auxiliar]\n",
        "df['Pergunta']= auxiliar2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXGehX0drJGx"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX4VGwykzk0R"
      },
      "source": [
        "todas_stopwords = stopwords.words('portuguese')\n",
        "todas_stopwords.append('para')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrCf_r8SkqDY"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "# inserir nova stopword\n",
        "# stopwords.append('existe')\n",
        "print(stopwords.words('portuguese'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dLbjxvNtbGz"
      },
      "source": [
        "# PROCURAR TEXTO PARA REMOVER STOPWORDS\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0v83jwTX9BK"
      },
      "source": [
        "# df = df.explode(\"Pergunta\", ignore_index=True)\n",
        "# df.rename(columns={\"Codigo\": \"Pergunta\"}, inplace=True)\n",
        "# df.index.name = \"Code_ID\"\n",
        "# df.head()\n",
        "# df.to_csv(\"Perguntas Tokenized.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5v1C238fiaE"
      },
      "source": [
        "#Para o caso de bag of words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxb8LkWTu-LU"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents\n",
        "vect = TfidfVectorizer(min_df=1, stop_words=todas_stopwords)\n",
        "tfidf = vect.fit_transform(df['Pergunta'][:5000])\n",
        "pairwise_similarity = tfidf * tfidf.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zXO0VZxNncm"
      },
      "source": [
        "#Estamos buscando os pares de pergunta similares, de acordo com o cálculo das diferenças entre os cosenos, buscando um limiar adequado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDAQVR6LvoKn"
      },
      "source": [
        "pairwise_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_QUvyygwL4v"
      },
      "source": [
        "pd.DataFrame(pairwise_similarity.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhyCgIf00bAM"
      },
      "source": [
        "#Armazenando o resultado em um Array\n",
        "resultado = pd.DataFrame(pairwise_similarity.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoDUX0k2P3WG"
      },
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "from scipy import stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rWg_TxJ_mfs"
      },
      "source": [
        "print (resultado)\n",
        "  \n",
        "#?? print (\"\\nclipped arr1 : \\n\", stats.threshold(resultado, threshmin = 0.7 , threshmax = 1, newval = 0.85))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpIDt-Ui0mzr"
      },
      "source": [
        "resultado.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t21GJAHWB_l4"
      },
      "source": [
        "resultado.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clzaii2J1AnE"
      },
      "source": [
        "resultado.to_excel('resultado.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ozCKpcsxEO6"
      },
      "source": [
        "df['Pergunta'][86], df['Pergunta'][514],df['Pergunta'][133], df['Pergunta'][112]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbGQ4Qguxr8v"
      },
      "source": [
        "# Definir um valor de threshold para classificar como semelhante\n",
        "\n",
        "\n",
        "# Remover as perguntas semelhantes do conjunto\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjSlUiz_yynE"
      },
      "source": [
        "\n",
        "resultado[(resultado > 0.99)].any(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8VhRuZ2uDh1"
      },
      "source": [
        "to_drop = []\n",
        "par_perguntas =[]\n",
        "\n",
        "for column in resultado.columns:\n",
        " for index in resultado.index:\n",
        "  if resultado.loc[index, column] > 0.95 and index != column:\n",
        "   to_drop.append([index,column])\n",
        "   par_perguntas.append([df['Pergunta'][index],df['Pergunta'][column]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYXDJBugwfKp"
      },
      "source": [
        "#Identificamos (1110/2) 555 pares com semelhança de 0.95, sugerindo que sejam removidas do banco por duplicidade.\n",
        "len(to_drop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKfH3lE3vDCr"
      },
      "source": [
        "#Identificando os pares\n",
        "print(to_drop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsvg9q9L7g6V"
      },
      "source": [
        "to_drop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SXOGqBD7SEp"
      },
      "source": [
        "par_perguntas[55]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLCK0BUWyhoD"
      },
      "source": [
        "#Identificação textual dos pares de perguntas similares\n",
        "par_perguntas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-jI-8dfxSJH"
      },
      "source": [
        "#identificação textual pontual entre pares\n",
        "df['Pergunta'][3779],df['Pergunta'][3914]\n",
        "                                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ-O6mUEyy6u"
      },
      "source": [
        "# Segunda parte do trabalho abordará a busca da similaridade entre as perguntas utilizando o modelo BERT para comparar os resultados de encontrados anteriormente na proposta  da etapa de Bag of words\n",
        "##Aplicação de modelo transformer para similaridade\n",
        "\n",
        "\n",
        "https://huggingface.co/neuralmind/bert-base-portuguese-cased\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IY-hO7cFZFN"
      },
      "source": [
        "import os\n",
        "! apt-get update -qq > /dev/null   \n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "!pip install nlu  pyspark==2.4.7 > /dev/null   \n",
        "\n",
        "import nlu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Ig16e_IKPq"
      },
      "source": [
        "# 2.1 Carregar o dataset de Perguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5Je4aNEIi4y"
      },
      "source": [
        "## Estamos alterando os nomes da coluna para manter a compatibilidade com o Modelo proposto para o BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNSSUWT9IFGR"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Load dataset to Pandas\n",
        "df= pd.read_excel('3 colunas de PERGUNTA.xlsx', usecols = \"A,C:C\")\n",
        "df= df.rename(columns={'Texto da Pergunta': 'Title'})\n",
        "df= df.rename(columns={'Codigo da Pergunta': 'Id'})\n",
        "df= df[:5000]\n",
        "max_r = 5000\n",
        "df = df.iloc[0:max_r]\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3CDRJ5-I27w"
      },
      "source": [
        "auxiliar = df['Title'].str.lower()\n",
        "df['Title']= auxiliar\n",
        "df.head(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nml3wQ1gJCnE"
      },
      "source": [
        "df.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehbi2aMuJJLF"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    try:\n",
        "      nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    except:\n",
        "      print(input_str)\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    return str(only_ascii)\n",
        "\n",
        "auxiliar = [remove_accents(sentence).replace('b\\'','') for sentence in df['Title']]\n",
        "auxiliar2 = [sentence.replace('\\'','') for sentence in auxiliar]\n",
        "df['Title']= auxiliar2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lek-LWQ6JThv"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "todas_stopwords = stopwords.words('portuguese')\n",
        "todas_stopwords.append('para')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUXDIQk1Jco5"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "# inserir nova stopword\n",
        "# stopwords.append('existe')\n",
        "print(stopwords.words('portuguese'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbavvKODJsBt"
      },
      "source": [
        "# 2.2. Incorporar perguntas com embeddings de frases de BERT \n",
        "\n",
        "Podemos incorporar o título ou o corpo da pergunta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjrlgUdUJxIB"
      },
      "source": [
        "pipe = nlu.load('embed_sentence.bert')\n",
        "#pipe = nlu.load('albert bert elmo electra xlnet pos')\n",
        "predictions = pipe.predict(df.Title, output_level='document')\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4w1lQqSKAFi"
      },
      "source": [
        "predictions.iloc[0,2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YS_SuvlK2cX"
      },
      "source": [
        "predictions.iloc[0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuUVWHw6LRTf"
      },
      "source": [
        "# 2.3 Como encontrar N sentenças mais semelhantes em um conjunto de dados para uma determinada sentença no conjunto de dados usando BERT\n",
        "Frases com pequenas distâncias entre seus embeddings serão consideradas semelhantes entre si."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVK0-QAsLUsY"
      },
      "source": [
        "## Calculate dinstance between all pairs of sentences in DF \n",
        "e_col = 'sentence_embedding_bert'\n",
        "\n",
        "def get_sim_df_for_iloc(sent_iloc, predictions=predictions,e_col=e_col, pipe=pipe):\n",
        "  # This function calculatse the distances for one sentences at  predictions[sent_iloc] to all other sentences in predictions using the embedding defined by e_col \n",
        "  # put embeddings in matrix\n",
        "  embed_mat = np.array([x for x in predictions[e_col]])\n",
        "  # calculate distance between every embedding pair\n",
        "  sim_mat = cosine_similarity(embed_mat,embed_mat)\n",
        "  print(\"Perguntas semelhantes a : \" + df.iloc[sent_iloc].Title)\n",
        "  # write sim scores to df\n",
        "  df['sim_score'] = sim_mat[sent_iloc]\n",
        "  return df \n",
        "sentence_to_compare=  19\n",
        "to_drop_bert = []\n",
        "par_perguntas_bert =[]\n",
        "\n",
        "sim_df_for_one_sent = get_sim_df_for_iloc(sentence_to_compare,predictions,e_col)\n",
        "sim_df_for_one_sent = sim_df_for_one_sent[sim_df_for_one_sent['sim_score']>0.97]\n",
        "sim_df_for_one_sent.sort_values('sim_score', ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qopL6QMOtex"
      },
      "source": [
        "# 2.4 Função de plotagem para traçar a distância entre uma frase no conjunto de dados e todas as outras frases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOmifdqcOvy5"
      },
      "source": [
        "def viz_sim_df_for_one_sent( sent_iloc=0, N = 514, sim_df_for_one_sent=sim_df_for_one_sent):\n",
        "  # Representa as N sentenças mais semelhantes em nosso dataframe para a sentença na posição sent_iloc\n",
        "  sim_df_for_one_sent = get_sim_df_for_iloc(sent_iloc)\n",
        "  \n",
        "  sim_df_for_one_sent.index = sim_df_for_one_sent.Title\n",
        "  sent = sim_df_for_one_sent.iloc[sent_iloc].Title\n",
        "  ax = sim_df_for_one_sent.sort_values('sim_score', ascending = False).iloc[:N].sim_score.plot.barh(title=f'As {N} frases mais semelhantes em nosso conjunto de dados para a pergunta \\ n\"{sent}\"', figsize=(40, 30))\n",
        "  ax.set_xlim(0.8, 1)\n",
        "\n",
        "# Basta inserir qualquer número e obter o gráfico de semelhanças da frase em df.iloc [i]\n",
        "# No caso escolhemos a mesma pergunta do título 2.3\n",
        "viz_sim_df_for_one_sent(19)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn3vjmJFPdce"
      },
      "source": [
        "#2.5 Calcular cada pontuação de similaridade entre cada frase no dataframe de entrada em pares e gere a matriz de similaridade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbxOxAFpPuHq"
      },
      "source": [
        "def get_sim_df_total( predictions,e_col, string_to_embed,pipe=pipe):\n",
        "  # Esta função calcula as distâncias entre cada par de perguntas. Cria para sempre a frase uma nova coluna com o nome igual à frase com a qual ela se compara\n",
        "\n",
        " \n",
        "  # put embeddings in matrix\n",
        "  embed_mat = np.array([x for x in predictions[e_col]])\n",
        "  # calcular a distância entre cada par de embedding\n",
        "  sim_mat = cosine_similarity(embed_mat,embed_mat)\n",
        "  # for i,v in enumerate(sim_mat): predictions[str(i)+'_sim'] = sim_mat[i]\n",
        "  for i,v in enumerate(sim_mat): \n",
        "    s = predictions.iloc[i].document\n",
        "    predictions[s] = sim_mat[i]\n",
        "\n",
        "  return predictions \n",
        "sim_matrix_df = get_sim_df_total(predictions,'sentence_embedding_bert', sentence_to_compare )\n",
        "sim_matrix_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg63NNLDP9Tg"
      },
      "source": [
        "#2.6 Plotar mapa de calor da matriz de similaridade para as primeiras N perguntas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUeaWSQvQJaj"
      },
      "source": [
        "non_sim_columns  = ['text','document','sentence_embedding_bert']\n",
        "\n",
        "def viz_sim_matrix_first_n(num_sentences=5, sim_df = sim_matrix_df):\n",
        "  # Plotar mapa de calor para as primeiras num_sentences\n",
        "  fig, ax = plt.subplots(figsize=(20,10)) \n",
        "  sim_df.index = sim_df.document\n",
        "  sim_columns = list(sim_df.columns)\n",
        "  for b in non_sim_columns : sim_columns.remove(b)\n",
        "  # sim_matrix_df[sim_columns]\n",
        "  ax = sns.heatmap(sim_df.iloc[:num_sentences][sim_columns[:num_sentences]]) \n",
        "\n",
        "  ax.axes.set_title(f\"Matriz de Similaridade para as {num_sentences} sentenças no dataset\",)\n",
        "\n",
        "viz_sim_matrix_first_n()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xr_UVMlRMQW"
      },
      "source": [
        "#2.7 Plotar mapa de calor da matriz de similaridade para as perguntas entre starT_iloc e end_iloc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9WVHiGdRQs5"
      },
      "source": [
        "def viz_sim_matrix_from_to(start_iloc,end_iloc, sim_df = sim_matrix_df):\n",
        "  # Plotar matriz térmica para sentenças em df.iloc [início: fim]   \n",
        "  fig, ax = plt.subplots(figsize=(50,30)) \n",
        "  sim_df.index = sim_df.document\n",
        "  sim_columns = list(sim_df.columns)\n",
        "  for b in non_sim_columns : sim_columns.remove(b)\n",
        "\n",
        "\n",
        "  ax = sns.heatmap(sim_df.iloc[start_iloc:end_iloc][sim_columns[start_iloc:end_iloc]]) # +2 because first 2 cols are not sim_scores\n",
        "\n",
        "  ax.axes.set_title(f\"Matriz de similaridade para as perguntas nas posições df.iloc[{start_iloc}:{end_iloc}] no dataset\",)\n",
        "\n",
        "viz_sim_matrix_from_to(10,20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVrgrCJ_RqZX"
      },
      "source": [
        "#2.8 Encontrar as N perguntas mais semelhantes em um conjunto de dados para uma nova pergunta que não existe nos dados usando BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKl1yp3mRxe7"
      },
      "source": [
        "def get_sim_df_for_string(predictions,e_col, string_to_embed,pipe=pipe):\n",
        "  # Cria um Dataframe que tem uma coluna sim_score que descreve a semelhança com a variável string_to_embed\n",
        "\n",
        "  # colocar vetores de predições na matriz\n",
        "  embed_mat = np.array([x for x in predictions[e_col]])\n",
        "\n",
        "  # inserir string de entrada\n",
        "  embedding = pipe.predict(string_to_embed).iloc[0][e_col]\n",
        "\n",
        "  # Replicar incorporação para string de entrada\n",
        "  m = np.array([embedding,]*len(df))\n",
        "  sim_mat = cosine_similarity(m,embed_mat)\n",
        "\n",
        "  #write sim score\n",
        "  df['sim_score'] = sim_mat[0]\n",
        "\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3kKwXU2R4aD"
      },
      "source": [
        "question = ' contribui para o seguro social?'\n",
        "sim_df = get_sim_df_for_string(predictions,'sentence_embedding_bert', 'existe ASO disponivel' )\n",
        "ax = sim_df.sort_values('sim_score', ascending = False).iloc[:20][['sim_score','Title']].plot.barh(title = f\"Frases mais semelhantes para pergunta\\n'{question}'\", figsize=(40,32))\n",
        "ax.set_xlim(0.8, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQYOhfnOSkf8"
      },
      "source": [
        "#2.8 Defina a função de plotagem do Helper para representar os resultados da incorporação de uma string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv5Tp6fKSoZs"
      },
      "source": [
        "def viz_sim_df_for_one_sent( question='foi feita a vistoria no andaime', e_col='sentence_embedding_bert', N = 40, sim_df_for_one_sent=sim_df_for_one_sent):\n",
        "  # Plots the N most similar sentences in our dataframe for sentence at position sent_iloc\n",
        "  sim_df = get_sim_df_for_string(predictions,e_col,question )\n",
        "  sim_df.index = sim_df.Title\n",
        "  sim_df.sort_values('sim_score', ascending = False).iloc[:N][['sim_score','Title']].plot.barh(title = f\"Most similar Sentences for sentence\\n'{question}'\", figsize=(20,14))\n",
        "  ax.set_xlim(0.8, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb7a3WZcS_7C"
      },
      "source": [
        "question = 'contribui para o seguro social?'\n",
        "e_col = 'sentence_embedding_bert'\n",
        "viz_sim_df_for_one_sent(question,e_col)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_04UlMceTReV"
      },
      "source": [
        "viz_sim_df_for_one_sent('seguro social?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3v86nAJGuIX"
      },
      "source": [
        "# 2.9 Multi Embedding Similarity, encontre as N perguntas mais semelhantes em um conjunto de dados para uma nova frase usando BERT, USE, Electra\n",
        "\n",
        "Primeiro, vamos carregar 3 embeddings ao mesmo tempo e incorporar o texto em nosso conjunto de dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhqKnY_IGzgJ"
      },
      "source": [
        "multi_pipe = nlu.load('use en.embed_sentence.electra embed_sentence.bert')\n",
        "multi_embeddings = multi_pipe.predict(df.Title,output_level='document')\n",
        "# multi_embeddings = multi_pipe.predict(df.Title)\n",
        "\n",
        "multi_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCohlO1IHDro"
      },
      "source": [
        "#2.10 Cálculo de similaridade de Multi Embeddings\n",
        "\n",
        "\n",
        "Vamos definir uma função que recebe uma string para incorporar, uma lista de embeddings e um pipeline\n",
        "\n",
        "get_sim_df_for_string_multi () calcula todos os embeddings carregados no pipeline NLU de entrada para a string de entrada e calcula as distâncias para cada frase no DF de entrada em todos os embeddings e nos dará uma pontuação normalizada final.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa34XxGtHKA6"
      },
      "source": [
        "def get_sim_df_for_string_multi(predictions,embed_col_names, string_to_embed,pipe=multi_pipe):\n",
        "  # Cria um Dataframe que tem uma coluna sim_score que descreve a semelhança com a variável string_to_embed\n",
        "  #Isso acumula as distâncias de todos os embeddings em embed_col_names e normaliza dividindo por len (embed_col_names)\n",
        "  #faça uma matriz de semelhança vazia que irá armazenar as semelhanças agregadas entre diferentes embeddings\n",
        "  predictions.dropna(inplace=True)\n",
        "  cum_sim = np.zeros((len(predictions),len(predictions)))\n",
        "\n",
        "  # embed with all embedders currently loaded in pipeline\n",
        "  embeddings = pipe.predict(string_to_embed).iloc[0]\n",
        "\n",
        "  #loop over all embeddings columns and accumulate the pairwise distances with string_to_embed into cum_sim\n",
        "  for e_col in embed_col_names:\n",
        "    # get the current embedding for input string\n",
        "    embedding = embeddings[e_col]  \n",
        "    # stack embedding vector for input string\n",
        "    m = np.array([embedding,]*len(predictions)) \n",
        "    # put df vectors in np matrix\n",
        "    embed_mat = np.array([x for x in predictions[e_col]]) \n",
        "    # calculate new similarities\n",
        "    sim_mat = cosine_similarity(m,embed_mat) \n",
        "  # accumulate new simmilarities in cum_sum\n",
        "    cum_sim += sim_mat  \n",
        "\n",
        "  predictions['sim_score'] = cum_sim[0]/len(embed_col_names) \n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfGbnChwI2Yf"
      },
      "source": [
        "question = 'contribui para o seguro social?'\n",
        "col_names = ['sentence_embedding_electra','sentence_embedding_bert', 'sentence_embedding_use']\n",
        "\n",
        "sim_df = get_sim_df_for_string_multi(multi_embeddings,col_names, question )\n",
        "sim_df.index = sim_df.document\n",
        "sim_df.sort_values('sim_score', ascending = False).iloc[:15][['sim_score','document']].plot.barh(title = f\"Most similar Sentences for sentence\\n'{question}'\", figsize=(20,14))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw9m1FA8JQ0U"
      },
      "source": [
        "# 2.11 Defina a função auxiliar para traçar os resultados de similaridade de uma string multiincorporada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjDfhEYBJdGd"
      },
      "source": [
        "def viz_sim_df_for_one_sent_multi_embed( question='Foi feita a vistoria no Guindaste', e_cols=col_names, N = 40, multi_embeddings=multi_embeddings):\n",
        "  # Representa as N sentenças mais semelhantes em nosso dataframe para a sentença na posição sent_iloc\n",
        "  sim_df = get_sim_df_for_string_multi(multi_embeddings,col_names, question )\n",
        "  sim_df.index = sim_df.document\n",
        "  sim_df.sort_values('sim_score', ascending = False).iloc[:N][['sim_score','document']].plot.barh(title = f\"Most similar Sentences for sentence\\n'{question}'\",figsize=(20,14))\n",
        "\n",
        "  ax.set_xlim(0.8, 1)\n",
        "\n",
        "question = 'contribui para o seguro social'\n",
        "col_names = ['sentence_embedding_electra','sentence_embedding_bert', 'sentence_embedding_use']\n",
        "viz_sim_df_for_one_sent_multi_embed(question, col_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R5aYw3TKCBT"
      },
      "source": [
        "question = 'ASO'\n",
        "viz_sim_df_for_one_sent_multi_embed(question)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7kQErFtKQ-n"
      },
      "source": [
        "question = 'treinamento'\n",
        "viz_sim_df_for_one_sent_multi_embed(question)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhDCdUsxzsm_"
      },
      "source": [
        "#from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "  \n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "#model = AutoModelForMaskedLM.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}